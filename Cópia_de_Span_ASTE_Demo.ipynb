{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patrickaraujo/Span_ASTE_Demo/blob/main/C%C3%B3pia_de_Span_ASTE_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhOe7lftyNCO",
        "outputId": "9864496d-2079-46c0-9d67-b0b2b5d2fec6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izKXA4b6-oIv",
        "outputId": "70841211-d6e0-4a19-cc26-2ee81c96234f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Span-ASTE'...\n",
            "remote: Enumerating objects: 194, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 194 (delta 55), reused 41 (delta 41), pack-reused 116\u001b[K\n",
            "Receiving objects: 100% (194/194), 615.10 KiB | 2.41 MiB/s, done.\n",
            "Resolving deltas: 100% (87/87), done.\n",
            "Note: switching to 'f53ec3c'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at f53ec3c Add command-line scoring instructions in README.md\n",
            "Collecting Cython==0.29.21 (from -r requirements.txt (line 1))\n",
            "  Downloading Cython-0.29.21-py2.py3-none-any.whl (974 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.2/974.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PYEVALB==0.1.3 (from -r requirements.txt (line 2))\n",
            "  Downloading PYEVALB-0.1.3-py3-none-any.whl (13 kB)\n",
            "Collecting allennlp-models==1.2.2 (from -r requirements.txt (line 3))\n",
            "  Downloading allennlp_models-1.2.2-py3-none-any.whl (353 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.7/353.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting allennlp==1.2.2 (from -r requirements.txt (line 4))\n",
            "  Downloading allennlp-1.2.2-py3-none-any.whl (505 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.1/505.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore==1.19.46 (from -r requirements.txt (line 5))\n",
            "  Downloading botocore-1.19.46-py2.py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fire==0.3.1 (from -r requirements.txt (line 6))\n",
            "  Downloading fire-0.3.1.tar.gz (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nltk==3.6.6 (from -r requirements.txt (line 7))\n",
            "  Downloading nltk-3.6.6-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.21.5 (from -r requirements.txt (line 8))\n",
            "  Downloading numpy-1.21.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.1.5 (from -r requirements.txt (line 9))\n",
            "  Downloading pandas-1.1.5.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pydantic==1.6.2 (from -r requirements.txt (line 10))\n",
            "  Downloading pydantic-1.6.2-py36.py37.py38-none-any.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.3/99.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==0.22.2.post1 (from -r requirements.txt (line 11))\n",
            "  Downloading scikit-learn-0.22.2.post1.tar.gz (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 0.2.0 Requires-Python ==3.6\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.7.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.7.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/chiayewken/Span-ASTE.git\n",
        "!cd Span-ASTE && git checkout f53ec3c\n",
        "!cp -a Span-ASTE/* .\n",
        "!echo boto3==1.16.46 >> requirements.txt\n",
        "!bash setup.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fire"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xletpNN9vDPF",
        "outputId": "6a677f5a-e896-484a-ecd2-5eacff3c5a93"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/88.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.4.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=b018af14a2166d84bf3e80a51323797bfa5b950fd310c7e8392d4c6bf43bb311\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-pTnCgDxcSQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a63e79d1-8255-4b9b-f1f9-e892b0de26d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens: ['Judging', 'from', 'previous', 'posts', 'this', 'used', 'to', 'be', 'a', 'good', 'place', ',', 'but', 'not', 'any', 'longer', '.']\n",
            "target: (10, 10)\n",
            "opinion: (9, 9)\n",
            "label: LabelEnum.negative\n",
            "\n",
            "tokens: ['We', ',', 'there', 'were', 'four', 'of', 'us', ',', 'arrived', 'at', 'noon', '-', 'the', 'place', 'was', 'empty', '-', 'and', 'the', 'staff', 'acted', 'like', 'we', 'were', 'imposing', 'on', 'them', 'and', 'they', 'were', 'very', 'rude', '.']\n",
            "target: (19, 19)\n",
            "opinion: (31, 31)\n",
            "label: LabelEnum.negative\n",
            "\n",
            "tokens: ['The', 'food', 'was', 'lousy', '-', 'too', 'sweet', 'or', 'too', 'salty', 'and', 'the', 'portions', 'tiny', '.']\n",
            "target: (1, 1)\n",
            "opinion: (3, 3)\n",
            "label: LabelEnum.negative\n",
            "target: (1, 1)\n",
            "opinion: (5, 6)\n",
            "label: LabelEnum.negative\n",
            "target: (1, 1)\n",
            "opinion: (8, 9)\n",
            "label: LabelEnum.negative\n",
            "target: (12, 12)\n",
            "opinion: (13, 13)\n",
            "label: LabelEnum.negative\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Data Exploration\n",
        "data_name = \"15res\" #@param [\"14lap\", \"14res\", \"15res\", \"16res\"]\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"aste\")\n",
        "from data_utils import Data\n",
        "\n",
        "path = f\"aste/data/triplet_data/{data_name}/train.txt\"\n",
        "data = Data.load_from_full_path(path)\n",
        "\n",
        "for s in data.sentences[:3]:\n",
        "    print(\"tokens:\", s.tokens)\n",
        "    for t in s.triples:\n",
        "        print(\"target:\", (t.t_start, t.t_end))\n",
        "        print(\"opinion:\", (t.o_start, t.o_end))\n",
        "        print(\"label:\", t.label)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pretrained SpanModel weights\n",
        "from pathlib import Path\n",
        "template = \"https://github.com/chiayewken/Span-ASTE/releases/download/v1.0.0/{}.tar\"\n",
        "url = template.format(data_name)\n",
        "model_tar = Path(url).name\n",
        "model_dir = Path(url).stem\n",
        "\n",
        "!wget -nc $url\n",
        "!tar -xf $model_tar"
      ],
      "metadata": {
        "id": "3LmrJekiPHpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3i4rnIhapWe"
      },
      "outputs": [],
      "source": [
        "# Use pretrained SpanModel weights for prediction\n",
        "import sys\n",
        "sys.path.append(\"aste\")\n",
        "from pathlib import Path\n",
        "from data_utils import Data, Sentence, SplitEnum\n",
        "from wrapper import SpanModel\n",
        "\n",
        "def predict_sentence(text: str, model: SpanModel) -> Sentence:\n",
        "    path_in = \"temp_in.txt\"\n",
        "    path_out = \"temp_out.txt\"\n",
        "    sent = Sentence(tokens=text.split(), triples=[], pos=[], is_labeled=False, weight=1, id=0)\n",
        "    data = Data(root=Path(), data_split=SplitEnum.test, sentences=[sent])\n",
        "    data.save_to_path(path_in)\n",
        "    model.predict(path_in, path_out)\n",
        "    data = Data.load_from_full_path(path_out)\n",
        "    return data.sentences[0]\n",
        "\n",
        "text = \"Did not enjoy the new Windows 8 and touchscreen functions .\"\n",
        "model = SpanModel(save_dir=model_dir, random_seed=0)\n",
        "sent = predict_sentence(text, model)\n",
        "\n",
        "for t in sent.triples:\n",
        "    target = \" \".join(sent.tokens[t.t_start:t.t_end+1])\n",
        "    opinion = \" \".join(sent.tokens[t.o_start:t.o_end+1])\n",
        "    print()\n",
        "    print(dict(target=target, opinion=opinion, sentiment=t.label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "srSNwqUz-39x"
      },
      "outputs": [],
      "source": [
        "# Train SpanModel from scratch\n",
        "random_seed = 4\n",
        "path_train = f\"aste/data/triplet_data/{data_name}/train.txt\"\n",
        "path_dev = f\"aste/data/triplet_data/{data_name}/dev.txt\"\n",
        "save_dir = f\"outputs/{data_name}/seed_{random_seed}\"\n",
        "\n",
        "model = SpanModel(save_dir=save_dir, random_seed=random_seed)\n",
        "model.fit(path_train, path_dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjyiKWjSF7oZ"
      },
      "outputs": [],
      "source": [
        "# Evaluate SpanModel F1 Score\n",
        "import json\n",
        "\n",
        "path_pred = \"pred.txt\"\n",
        "path_test = f\"aste/data/triplet_data/{data_name}/test.txt\"\n",
        "model.predict(path_in=path_test, path_out=path_pred)\n",
        "results = model.score(path_pred, path_test)\n",
        "print(json.dumps(results, indent=2))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}